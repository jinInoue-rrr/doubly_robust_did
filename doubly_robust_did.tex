\documentclass{beamer}
\usepackage{luatexja} 
\usepackage[ipaex]{luatexja-preset} 
\renewcommand{\kanjifamilydefault}{\gtdefault} 

\usepackage{bm} 
\usepackage{mathtools} 
\usepackage{amsmath} 
\usepackage{amsthm}
\usepackage{setspace}

\newtheorem{thm}{Theorem}[section] 
\newtheorem{lem}[thm]{Lemma} 
\newtheorem{prop}[thm]{Proposition} 
\newtheorem{assumption}[thm]{Assumption} 
\usetheme{Madrid} 
\setbeamertemplate{navigation symbols}{} 

\DeclareMathOperator*{\argmax}{arg\,\max}
\DeclareMathOperator*{\argmin}{arg\,\min}
\renewcommand{\baselinestretch}{0.95}

\title{Introduction to Doubly Robust DiD} 
\subtitle{Sant'Anna and Zhao(2020)}
\author{M1 Inoue Jin}
\institute{Hitotsubashi University} 

\begin{document}
%\setlength{\mathindent}{0pt}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}

\begin{frame}\frametitle{}
    \titlepage
\end{frame}

\begin{frame}\frametitle{Doubly Robust DiD}
    \begin{itemize}
        \item Doubly Robust DiD was proposed by Sant'Anna and Zhao(2020)
        \item This paper suggests doubly robust estimators for the ATT in DiD research designs.
        \item Authors also state that one can construct doubly robust DiD estimators for the ATT that are also doubly robust for inference.
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Notation}
    \begin{itemize}
        \item $Y_{it}$ : the outcome of interest for unit $i$ at time $t$
        \item $t$ : researchers have access to outcome data in a pre-treatment period $t = 0$ and in a post-treatment period $t = 1$.
        \item $D_{it} = 1$ if $i$ is treated before time t and $D_{it} = 0$ otherwise.
        \begin{itemize}
            \item $D_{i0} = 0$ for every $i$ at time $t$ and then, $D_{i} = D_{i1}$
        \end{itemize}
        \item $Y_{it} = D_{i}Y_{it}(1) + (1 - D_{i})Y_{it}(0)$
        \begin{itemize}
            \item $Y_{i0} = Y_{i0}(0)$ for all $i$ 
            \item $Y_{i1} = D_{i}Y_{i1}(1) + (1 - D_{i})Y_{i1}(0)$
        \end{itemize}
        \item $X_{i}$ :a vector of pre-treatment covariates.
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{The parameter of interest}
    \begin{itemize}
        \item Difference in Differences identifies the average treatment effect for the treated (ATT). 
    \end{itemize}
    \begin{block}{The parameter of interest:ATT}
        $\tau = \mathbb{E}[Y_{i1}(1) - Y_{i1}(0)| D_{i} = 1] = \mathbb{E}[Y_{1} |D = 1] - \mathbb{E}[Y_{1}(0) | D = 1]$
    %Y_{i1}(1) = Y_{i1} if D_{i} = 1
    \end{block}
\end{frame}

\begin{frame}\frametitle{Assumption 1.}
    \begin{block}{Assumption 1. (Random Sampling)}
        the data $\{Y_{i0}, Y_{i1}, D_{i}, X_{i}\}^{n}_{i = 1}$ are independent and identically distributed (iid)
    \end{block}
\end{frame}

\begin{frame}\frametitle{Assumption 2.}
    \begin{block}{Assumption 2. (Conditional PTA)}
        $\mathbb{E}[Y_{1}(0) - Y_{0}(0)| D = 1, X] = \mathbb{E}[Y_{1}(0) - Y_{0}(0)|D = 0, X]$ almost surely (a.s.).
    \end{block}
    \begin{itemize}
        \item This assumption is called "Conditional Parallel Trend Assumption"
        \item It is crucial for most of DiD literature.
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Assumption 3.}
    \begin{block}{Assumption 3. (Overlap Condition)}
        For some $\varepsilon > 0$, $\mathbb{P}(D = 1) > \varepsilon$ and $\mathbb{P}(D = 1|X) \leq 1 - \varepsilon$ a.s.
    \end{block}
    \begin{itemize}
        \item This assumption states that at least a small fraction of the population is treated and that for every value of the covariates X, there is at least a small probability that the unit is not treated.
        \item These assumptions are standard in conditional DID methods.
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Doubly Robust DiD estimand:Notation}
    \begin{itemize}
        \item $\pi(X)$ : an arbitrary model for the true, unknown propensity score
        \item $\Delta Y = Y_{1} - Y_{0}$:the difference of observed outcomes
        \item $\mu ^{p}_{d,\Delta} \equiv \mu ^{p}_{d,1}(X) - \mu ^{p}_{d,0}(X)$
        \begin{itemize}
            \item $\mu ^{p}_{d,t}$ : a model for the true, unknown outcome regression $m^{p}_{d,t} \equiv \mathbb{E}[Y_{t}|D = d, X = x]],d,t = 0,1$
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Doubly Robust DiD estimand}
    \begin{block}{DR DID estimand when panel data are available}
        \begin{equation*}
        \tau^{dr,p} = \mathbb{E}\lbrack (w^{p}_{1}(D)- w^{p}_{0}(D,X;\pi))(\Delta Y - \mu ^{p}_{0,\Delta}(X)))\rbrack
        \end{equation*}
        where, for a generic $g$,
        \begin{equation*}
            w^{p}_{1}(D) = \frac{D}{ \mathbb{E}[D]},w^{p}_{0}(D,X;g) = \frac{ \frac{g(X)(1-D)}{1-g(X)}}{ \mathbb{E}[\frac{g(X)(1-D)}{1-g(X)}]}
        \end{equation*}
    \end{block}
\end{frame}

\begin{frame}\frametitle{Doubly Robust DiD estimand}
    \begin{block}{Theorem 1.}
        Let Assumptions 1-3 hold. When panel data are available, $\tau ^{dr,p} = \tau$ if either (but not necessarily both) $\pi(X) = p(X)$ a.s. or $\mu ^{p}_{\Delta}(X) = m^{p}_{0,1}(X) - m^{p}_{0,0}(X)$
    \end{block}

    \begin{itemize}
        \item This theorem states that at least one of the outcome regression model or propensity score model is correctly specified, we can recover the average treatment effect for the treated(ATT).
        \begin{itemize}
            \item Case1:the propensity score model is correctly specified($\pi(X) = p(X) = P(D = 1|X)$), but the outcome regression model is miss-specified.
            \item Case2:the outcome regression model is correcly specified($\mu ^{p}_{0,\Delta}(X) = m^{p}_{0,\Delta}(X) = \mathbb{E}[Y_{1}|D = 0, X] - \mathbb{E}[Y_{0}|D = 0, X]$), but the propensity score model is miss-specified.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Proof of Theorem 1.(Case 1)}
    In Case 1, the propensity score model $\pi(X)$ is equivalent to the true propensity score $p(X) = P(D = 1|X)$. 

    To begin with, calculate the difference of two weights $w^{p}_{1}(D) - w^{p}_{0}(D,X)$:
    \begin{eqnarray*}
        w^{p}_{1}(D) - w^{p}_{0}(D,X) &=& \frac{D}{ \mathbb{E}[D]} - \frac{ \frac{\pi(X)(1-D)}{1-\pi(X)}}{ \mathbb{E}[\frac{\pi(X)(1-D)}{1-\pi(X)}]} \\
        &=& \frac{D}{\mathbb{E}[D]} - \frac{\frac{p(X)(1-D)}{1-p(X)}}{\mathbb{E}[D]} \\
        &=& \frac{1}{\mathbb{E}[D]} \left(\frac{(1-p(X))D}{1-p(X)} - \frac{(1-D)p(X)}{1-p(X)} \right) \\
        &=& \frac{D - p(X)}{\mathbb{E}[D](1-p(X))}
    \end{eqnarray*}
\end{frame}

\begin{frame}\frametitle{Proof of Theorem 1.(Case 1)}
Therefore,
    \begin{eqnarray*}
        \tau ^{dr,p} &=& \mathbb{E}\left[(w^{p}_{1}(D) - w^{p}_{0}(D,X))(\Delta Y - \mu^{p}_{0,\Delta}(X))\right] \\
        &=& \mathbb{E}\left[\left(\frac{D - p(X)}{\mathbb{E}[D](1-p(X))}\right)\left(\Delta Y - \mu^{p}_{0,\Delta}(X)\right)\right] \\
        &=& \mathbb{E}\left[\left(\frac{D - p(X)}{\mathbb{E}[D](1-p(X))}\right)\Delta Y\right] - \mathbb{E}\left[\left(\frac{D - p(X)}{\mathbb{E}[D](1-p(X))}\right)\mu^{p}_{0,\Delta}(X)\right]
    \end{eqnarray*}
The first term is equivalent to the Abadie(2005)'s IPW DID estimator. The second term seems to be "bias term" and we want this term to be zero.
\end{frame}

\begin{frame}\frametitle{Proof of Theorem 1.(Case 1)}
    Then, the bias term is:
    \begin{eqnarray*}
        \mathbb{E}\left[\left(\frac{D-p(X)}{\mathbb{E}[D](1-p(X))}\right)\mu^{p}_{0, \Delta}(X)\right] &=& \mathbb{E}\left[\mathbb{E}\left[\frac{\left(D-p(X)\right)\mu^{p}_{0,\Delta}(X)}{\mathbb{E}[D](1-p(X))}\middle| X\right]\right] \\
        &=& \mathbb{E}\left[\frac{\mu^{p}_{0,\Delta}(X)\mathbb{E}\left[(D-p(X))\middle|X\right]}{\mathbb{E}[D](1-p(X))}\right]\\
        &=& \mathbb{E}\left[\frac{\mu^{p}_{0,\Delta}(X) \left(\mathbb{E}\left[D\middle|X\right] - p(X)\right)}{\mathbb{E}\left[D\right](1-p(X))}\right] \\
        &=& 0
    \end{eqnarray*}
The first line is obtained by the law of iterated expectation, and the third line reduces to 0 because $p(X) = P(D = 1|X) = \mathbb{E}[D|X]$.
\end{frame}

\begin{frame}\frametitle{Proof of Theorem 1.(Case 1)}
    Next, check whether the IPW estimator is equivalent to ATT:
    \begin{eqnarray*}
        \mathbb{E}\left[\frac{(D-p(X))}{\mathbb{E}[D](1-p(X))}\Delta Y \right] &=&\mathbb{E}\left[\mathbb{E}\left[\mathbb{E}\left[\frac{(D-p(X))}{\mathbb{E}[D](1-p(X))}\Delta Y\middle|D,X\right]\middle|X\right]\right] \\
    \end{eqnarray*}

    Note that $\Delta Y = Y_{1} - Y_{0} = DY_{1}(1) + (1-D)Y_{1}(0) - Y_{0}(0)$ (applying the equation of potential outcomes.) \\
\end{frame}

\begin{frame}\frametitle{Proof of Theorem 1.(Case 1)}
    We can calculate the second expectation by definition:
    \footnotesize
    \begin{align*}
        \MoveEqLeft
        \mathbb{E}_{D}\left[\mathbb{E}\left[\frac{D-p(X)}{\mathbb{E}[D](1-p(X))}\left(DY_{1}(1) + (1-D)Y_{1}(0) - Y_{0}(0)\right)\middle|D,X\right]\middle|X \right]
        \\&= 
        \frac{p(X)}{P(D = 1)}\mathbb{E}\left[Y_{1}(1) - Y_{0}(0)\middle|D = 1, X\right] - \frac{p(X)}{P(D = 1)}\mathbb{E}\left[Y_{1}(0) - Y_{0}(0)\middle|D = 0, X \right]
        \\&= 
        \frac{p(X)}{P(D = 1)} \{ \mathbb{E}[Y_{1}(1) - Y_{0}(0)|D = 1, X] - \mathbb{E}[Y_{1}(0) - Y_{0}(0)|D = 1, X] \}
        \\&=
        \frac{p(X)}{P(D = 1)}\mathbb{E}[Y_{1}(1) - Y_{1}(0)|D = 1, X]
    \end{align*}
    \normalsize
    The first equality follows from the direct calculation of the outer conditional expectation, and we can obtain the second equality by applying the conditional PTA(Assumption 2.).
\end{frame}

\begin{frame}\frametitle{Proof of Theorem 1.(Case 1.)}
    Finally, ATT is recovered by direct calculation.
    \begin{align*}
        \MoveEqLeft
        \mathbb{E}_{X}\left[\frac{p(X)}{P(D = 1)}\mathbb{E}[Y_{1}(1) - Y_{1}(0)|D = 1, X]\right]
        \\&=
        \int_{x}\frac{P(D = 1|X)}{P(D = 1)} \int_{y} \left(y_{1}(1) - y_{1}(0)\right)f(y|d = 1, x)f(x)dydx
        \\&=
        \int_{y}\int_{x}\frac{f(d = 1|x)}{f(d = 1)}\frac{f(y, d = 1, x)}{f(d = 1, x)}f(x)\left(y_{1}(1) - y_{1}(0)\right)dxdy
        \\&=
        \int_{y}\int_{x}\frac{f(d = 1|x)}{f(d = 1)}\frac{f(y, d = 1, x)f(x)}{f(d = 1 |x)f(x)}\left(y_{1}(1) - y_{1}(0)\right)dxdy
        \\&=
        \int_{y}\frac{y_{1}(1) - y_{1}(0)}{f(d = 1)} \biggl \{ \int_{x}f(y, d = 1, x)dx \biggl \} dy
        \\&=
        \int_{y}(y_{1}(1) - y_{1}(0))f(y|d = 1)dy
        \\&=
        \mathbb{E}[Y_{1}(1) - Y_{1}(0)|D = 1]
    \end{align*}
\end{frame}

\begin{frame}\frametitle{Estimation and Inference}
    \begin{block}{DR DiD estimator}
        \begin{equation*}
            \widehat{\tau}^{dr,p} = \mathbb{E}_{n}\left[(\widehat{w}^{p}_{1}(D) - \widehat{w}^{p}_{0}(D,X;\widehat{\gamma}))(\Delta Y - \mu^{p}_{0, \Delta}(X;\widehat{\beta}^{p}_{0,0},\widehat{\beta}^{p}_{0,1}))\right],
        \end{equation*}
        where\\
        $\widehat{w}^{p}_{1}(D) = \frac{D}{\mathbb{E}_{n}[D]}$, $\widehat{w}^{p}_{0}(D,X;\gamma) = \left. \frac{\pi(X;\gamma)(1-D)}{1-\pi(X;\gamma)}\middle/ \mathbb{E}_{n}\left[\frac{\pi(X;\gamma)(1-D)}{1-\pi(X;\gamma)}\right]\right.$
    \end{block}
    \begin{itemize}
        \item $\widehat{\tau}^{dr,p}$ is doubly robust, and also locally semiparametrically efficient when the working models for the nuisance functions are correctly specified.(Theorem A.1 in Appendix A)
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Semiparametric efficiency bound}
    \footnotesize
    \begin{itemize}
        \item This result provides the semiparametric analogue of the Cram\'{e}r-Rao lower bound commonly used in fully parametric procedures.
        \item Thus, this provides a benchmark that researchers can use to assess whether any given (regular) semiparametric DiD estimator works well.
    \end{itemize}
    \normalsize
    \begin{block}{Proposition 1}
        \footnotesize
        Let Assumptions 1-3 hold. Then, when panel data are available, the efficient influence function for the ATT is 
        \normalsize
        \footnotesize
        \begin{align*}
            \eta^{e,p}(Y_{1}, Y_{0}, D, X) &= w^{p}_{1}(D)(m^{p}_{1, \Delta}(X) - m^{p}_{0. \Delta}(X) - \tau) \\
            & + w^{p}_{1}(D)(\Delta Y - m^{p}_{1, \Delta}(X)) - w^{p}_{0}(D,X;p)(\Delta Y - m^{p}_{0, \Delta}(X)),
        \end{align*}
        \normalsize
        \footnotesize
        and the semiparametric efficiency bound for all regular estimators for the ATT is 
        \normalsize
        \footnotesize
        \begin{align*}
            \begin{split}
                \mathbb{E}[\eta^{e,p}(Y_{1}, Y_{0}, D, X)^{2}] &= \frac{1}{\mathbb{E}[D]^{2}}\mathbb{E} \biggr[ D(m^{p}_{1, \Delta}(X) - m^{p}_{0,\Delta}(X) - \tau)^{2} 
                \\&+ \left. D(\Delta Y - m^{p}_{1, \Delta}(X))^{2} + \frac{(1-D)p(X)^{2}}{(1-p(X))^{2}}(\Delta Y - m^{p}_{0. \Delta}(X))^{2} \right]
            \end{split}
        \end{align*}
        Note that $m^{p}_{0.\Delta}(X) \equiv m^{p}_{0,1}(X) - m^{p}_{0,0}(X)$
        \normalsize
    \end{block}
\end{frame}


%improved estimator 
\begin{frame}\frametitle{Estimation and Inference}
\begin{itemize}
    \item Practitioners need to choose a particular estimation procedure to be implemented.
    \item In this paper, authors propose the improved DR DiD estimator that can be easily implemented and is not only doubly robust for consistency but also doubly robust for inference.
    \item  Doubly Robustness for inference practically means that the asymptotic variance of the DR DiD estimator for the ATT is invariant to which working models for the nuisance functions are correctly specified.
    \item In the following derivation of the improved DR DiD estimators, we consider the case where we use linear model for the outcome regression, logistic model for the propensity score, and use covariates $X$ for all the nuisance models in a symmetric manner.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Estimation and Inference}
    As discussed above, we consider the following working models for the nuisance functions:
    \begin{align*}
        \pi(X;\gamma) = \Lambda(X'\gamma) \equiv \frac{exp(X'\gamma)}{1 + exp(X'\gamma)},\,\text{and} \, \mu^{p}_{0, \Delta} = \mu^{lin, p}_{0, \Delta} \equiv X'\beta^{p}_{0,\Delta}.
    \end{align*}
    \begin{block}{Improved DR DiD estimator}
        \begin{align*}
        \widehat{\tau}^{dr,p}_{imp} = \mathbb{E}_{n}[(\widehat{w}^{p}_{1}(D) - \widehat{w}^{p}_{0}(D,X;\widehat{\gamma}^{ipt}))(\Delta Y - \mu^{lin, p}_{0, \Delta}(X;\widehat{\beta}^{wls, p}_{0, \Delta}))]
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}\frametitle{Estimation and Inference}
    the improved DR DiD estimator consists of three estimation procedures. The first two-steps consist of computing the optimization problems stated below.
    \begin{align*}
        \widehat{\gamma}^{ipt} &= \argmax_{\gamma \in \Gamma} \mathbb{E}_{n}[DX'\gamma - (1-D)exp(X'\gamma)], \\
        \widehat{\beta}^{wls, p}_{0, \Delta} &= \argmin_{b \in \Theta} \mathbb{E}_{n}\left[\frac{\Lambda(X'\widehat{\gamma}^{ipt})}{1 - \Lambda(X'\widehat{\gamma}^{ipt})}(\Delta Y - X'b)^{2} \middle|D = 0 \right],
    \end{align*}
    Then, in the third step, one plugs the fitted values of the logit model and linear model into the sample analogue of $\tau^{dr, p}$ \\
    Here, note that $\widehat{\gamma}^{ipt}$ is the inverse probability tilting estimator proposed by Graham et al.(2012) in a different context.
\end{frame}

\begin{frame}\frametitle{Estimation and Inference}
    \begin{block}{Theorem 2.}
        \footnotesize
        Suppose Assumptions 1-3 and Assumption A.1-A.2 stated in Appendix A hold, and that the working models are logit model and linear regression model. Then,
        \\(a) If either $\Lambda(X'\gamma^{\ast,ipt}) = p(X)$ a.s. or $X'\beta^{\ast, wls, p}_{0,\Delta} = m^{p}_{0,\Delta}(X)$ a.s., then, as $n \rightarrow \infty$,
        $$
            \widehat{\tau}^{dr,p}_{imp} \overset{p}{\to} \tau
        $$
        and
        \normalsize
        \footnotesize
        \begin{align*}
            \sqrt{n}(\widehat{\tau}^{dr,p}_{imp} - \tau^{dr,p}_{imp})
            &= 
            \frac{1}{\sqrt{n}}\sum_{i = 1}^{n} \eta^{dr,p}_{imp}(W;\gamma^{\ast,ipt}, \beta^{\ast,wls,p}_{0,\Delta},\tau^{dr,p}_{imp}) + o_{p}(1)
            \\ & \overset{d}{\to} N(0,V^{p}_{imp}),
        \end{align*}
        \normalsize
        \footnotesize
        where $V^{p}_{imp} = \mathbb{E}\left[\eta^{dr,p}_{imp}(W;\gamma^{\ast, ipt},\beta^{\ast, wls, p}_{0, \Delta}, \tau^{dr, p}_{imp})^{2}\right]$.
        \normalsize
        \footnotesize
        \\(b) If both $\Lambda(X'\gamma^{\ast,ipt}) = p(X) a.s.$ and $X'\beta^{\ast, wls, p}_{0, \Delta} = m^{p}_{0, \Delta}(X) a.s.$, then $\eta^{dr, p}_{imp}(W;\gamma^{\ast, ipt},\beta^{\ast, wls, p}, \tau^{dr,p}_{imp}) = \eta^{e,p}(Y_{1}, Y_{0}, D, X) a.s.$ and $V^{p}_{imp}$ is equal to the semiparametric efficient bound. 
        \normalsize
    \end{block}
\end{frame}


\begin{frame}\frametitle{Monte Carlo simulation study}
    \begin{itemize}
        \item Monte Carlo experiments in order to study the finite sample property of DiD estimators including DR DiD estimator.
        \item $\widehat{\tau}^{dr, p}$, $\widehat{\tau}^{dr,p}_{imp}$:DR DiD estimators.
        \item $\widehat{\tau}^{reg} = \bar{Y}_{1,1} - \left[\bar{Y}_{1,0}  + \frac{1}{n_{treat}}\sum_{i|D_{i} = 1}(\widehat{\mu}_{0,1}(X_{i}) - \widehat{\mu}_{0,0}(X_{i}))\right],$ where $\bar{Y}_{d,t} = \sum_{i|D_{i} = d, T_{i} = t}Y_{it}/n_{d,t}$ is the sample average outcome among units in treatment group d and time t, see e.g. Heckman et al.(1997).
        \item $\widehat{\tau}^{ipw,p} = \frac{1}{\mathbb{E}_{n}[D]}\mathbb{E}_{n}\left[\frac{D-p(X)}{1-p(X)}(Y_{1} - Y_{0})\right]$ see e.g. Abadie(2005).
        \item $\widehat{\tau}^{ipw,p}_{std} = \mathbb{E}_{n}[(\widehat{w}^{p}_{1}(D) - \widehat{w}^{p}_{0}(D,X;\widehat{\gamma}))(Y_{1} - Y_{0})]$
        \item $\widehat{\tau}^{fe}$:Two way fixed effect model.
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Monte Carlo simulation study}
    \begin{itemize}
        \item All observed covariates enter the working models linearly.
        \item n = 1000 for each design and conduct 10000 Monte Carlo simulations.
    \end{itemize}
    \footnotesize
    \begin{block}{Simulation design when panel data are available}
        For a generic $W = (W_{1}, W_{2}, W_{3}, W_{4})',$ let
        \begin{align*}
            f_{reg}(W) = 210 + 27.4\cdot W_{1} + 13.7\cdot(W_{2} + W_{3} + W_{4}),\\
            f_{ps}(W) = 0.75\cdot (-W_{1} + 0.5 \cdot W_{2} - 0.25 \cdot W_{3} - 0.1 \cdot W_{4}).
        \end{align*}
        Let $\bm{X} = (X_{1}, X_{2}, X_{3}, X_{4})'$ be distributed as $N(\bm{0}, I_{4}),$ and $I_{4}$ be the $4 \times 4$ identity matrix. For $j = 1,2,3,4,$ let $Z_{j} = \left. (\tilde{Z}_{j} - \mathbb{E}[\tilde{Z}_{j}])\middle/ \sqrt{Var(\tilde{Z})} \right.$, where $\tilde{Z}_{1} = exp(0.5X_{1}), \tilde{Z}_{2} = 10 + X_{2}/(1 + exp(X_{1})), \tilde{Z}_{3} = (0.6 + X_{1}X_{2}/25)^{3}$ and $\tilde{Z}_{4} = (20 + X_{2} + X_{4})^{2}$.
    \end{block}
    \normalsize
\end{frame}

\begin{spacing}{0.5}
\begin{frame}\frametitle{Monte Carlo simulation study}
    \footnotesize
    \begin{block}{Data generating process(DGP)}
        \begin{align*}
            Y_{0}(0) &= f_{reg}(Z) + v(Z, D) + \varepsilon_{0}, & Y_{1}(d) &= 2 \cdot f_{reg}(Z) + v(Z, D) + \varepsilon_{1}(d),d = 0,1, \\[-2pt]
            p(Z) &= \frac{exp(f_{ps}(Z))}{1 + exp(f_{ps}(Z))}, & D &= 1\{p(Z)\geq U\}:DGP1
        \end{align*}
        \\
        \begin{align*}
            Y_{0}(0) &= f_{reg}(Z) + v(Z, D) + \varepsilon_{0}, & Y_{1}(d) &= 2 \cdot f_{reg}(Z) + v(Z, D) + \varepsilon_{d}, d = 0,1, \\[-2pt]
            p(X) &= \frac{exp(f_{ps}(X))}{1 + exp(f_{ps}(X))}, & D &= 1\{p(X) \geq U\}:DGP2
        \end{align*}
        \\
        \begin{align*}
            Y_{0}(0) &= f_{reg}(X) + v(X, D) + \varepsilon_{0}, & Y_{1}(d) &= 2 \cdot f_{reg}(X) + v(X, D) + \varepsilon_{1}(d), d = 0,1,\\[-2pt]
            p(Z) &= \frac{exp(f_{ps}(Z))}{1 + exp(f_{ps}(Z))}, & D &= 1\{p(Z) \geq U\}:DGP3
        \end{align*}
        \\
        \begin{align*}
            Y_{0}(0) &= f_{reg}(X) + v(X, D) + \varepsilon_{0}, & Y_{1}(d) &= 2 \cdot f_{reg}(X) + v(X, D) + \varepsilon_{1}(d), d = 0,1,\\[-2pt]
            p(X) &= \frac{exp(f_{ps}(X))}{1 + exp(f_{ps}(X))}, & D &= 1\{p(X) \geq U\}:DGP4
        \end{align*}
    \end{block}
    \normalsize
\end{frame}
\end{spacing}

\begin{frame}\frametitle{Monte Carlo simulation study}
    \begin{figure}
        \centering
        \includegraphics[width = 11cm]{doublyrobustDD_sim.jpeg}
    \end{figure}
\end{frame}

%Appendix

\begin{frame}\frametitle{Appendix A}
    \footnotesize
    \begin{block}{Assumption A.1.}
    ($i$) $g(x) = g(x;\theta)$ is a parametric model, where $\theta \in \Theta \subset \mathbb{R}^{k},$ $\Theta$ being compact; ($ii$) $g(x;\theta)$ is $a.s.$ continuous at each $\theta \in \Theta$; ($iii$) there exists a unique pseudo-true parameter $\theta^{\ast} \in int(\Theta)$; ($iv$) $g(x;\theta)$ is $a.s.$ twice continuously differentiable in a neighborhood of $\theta^{\ast}, \Theta^{\ast} \subset \Theta$; ($v$) the estimator $\widehat{\theta}$ is strongly consistent for the $\theta^{\ast}$ and satisfies the following linear expansion:
    \footnotesize
    \begin{equation*}
        \sqrt{n}(\widehat{\theta} - \theta^{\ast}) = \frac{1}{\sqrt{n}} \sum_{i = 1}^{n}l_{g}(W_{i};\theta^{\ast}) + o_{p}(1),
    \end{equation*}
    \normalsize
    \footnotesize
    where $l_{g}(\cdot;\theta)$ is such that $\mathbb{E}[l_{g}(W;\theta^{\ast})] = 0$, $\mathbb{E}[l_{g}(W;\theta^{\ast})l_{g}(W;\theta^{\ast})']$ exists and is positive definite and $ lim_{\delta \to \infty} \mathbb{E}\left[\sup_{\theta \in \Theta^{\ast}:\|\theta-\theta^{\ast}\| \leq \delta}\|l_{g}(W;\theta)-l_{g}(W;\theta^{\ast})\|^{2} \right] = 0$. In addition, ($vi$) for some $\varepsilon > 0$, $0 < \pi(X;\gamma) \leq 1 - \varepsilon \,a.s.$, for all $\gamma \in int(\Theta^{ps})$,where $\Theta^{ps}$ denotes the parameter space of $\gamma$.
    \normalsize
    \end{block}
    \normalsize
\end{frame}

\begin{frame}\frametitle{Appendix A}
    \begin{block}{Assumption A.2.}
        When panel data are available, assume that $\mathbb{E}[\|h^{p}(W;\kappa^{\ast, p})\|^{2}] < \infty$ and $\mathbb{E}[sup_{\kappa \in \Gamma^{\ast, p}}|\dot{h}^{p}(W;\kappa)|] < \infty$,where $\Gamma^{\ast, p}$ is a small neighborhood of $\kappa^{\ast, p}$.
    \end{block}
    %some notations need to be explained,i.e.,\eta^p \kappa,...
    %dot{h} also need to be explained...
\end{frame}

\begin{frame}\frametitle{Appendix A}
    \footnotesize
    \begin{block}{Theorem A.1.}
        Suppose Assumptions 1-3 and Assumptions A.1-A.2 stated in Appendix A hold.
        Consider the following claims:
        \footnotesize
        \begin{align*}
            &\exists \gamma^{\ast} \in \Theta^{ps}:\mathbb{P}(\pi(X;\gamma^{\ast}) =p(X)) = 1,\\
            &\exists (\beta^{\ast, p}_{0,1}, \beta^{\ast, p}_{0,0}) \in \Theta^{reg}_{0,1} \times \Theta^{reg}_{0,0}:\mathbb{P}(\mu^{p}_{0,1}(X;\beta^{\ast,p}_{0,1}) - \mu^{p}_{0,0}(X;\beta^{\ast,p}_{0,0}) = m^{p}_{0,1}(X) - m^{p}_{0,0}(X)) = 1.
        \end{align*}
        \normalsize
        \footnotesize
        (a) Provided that either one of claims stated above is true, as $n \to \infty$,
        \begin{equation*}
            \widehat{\tau}^{dr,p} \overset{p}{\to} \tau.
        \end{equation*}
        Furthermore,
        \normalsize
        \footnotesize
        \begin{align*}
            \sqrt{n}(\widehat{\tau}^{dr,p} - \tau^{dr,p}) &= \frac{1}{\sqrt{n}}\sum_{i = 1}^{n}\eta^{p}(W_{i};\gamma^{\ast},\beta^{\ast,p}_{0}) + o_{p}(1)
            \\& \overset{d}{\to} N(0,V^{p}),
        \end{align*}
        \normalsize
        \footnotesize
        where $V^{p} = \mathbb{E}[\eta^{p}(W;\gamma^{\ast},\beta^{\ast,p}_{0})^{2}]$.\\
        (b) When both of the claims above are true, $\eta^{p}(W;\gamma^{\ast},\beta^{\ast, p}_{0}) = \eta^{e,p}(Y_{1}, Y_{0}, D, X) a.s.$ and $V^{p}$ is equal to the semiparametric efficient bound.
        \normalsize
    \end{block}
    \normalsize
\end{frame}

\end{document}