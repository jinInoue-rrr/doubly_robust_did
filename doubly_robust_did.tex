\documentclass{beamer}
\usepackage{luatexja} 
\usepackage[ipaex]{luatexja-preset} 
\renewcommand{\kanjifamilydefault}{\gtdefault} 

\usepackage{bm} 
\usepackage{mathtools} 
\usepackage{amsmath} 
\usepackage{amsthm}

\newtheorem{thm}{Theorem}[section] 
\newtheorem{lem}[thm]{Lemma} 
\newtheorem{prop}[thm]{Proposition} 
\newtheorem{assumption}[thm]{Assumption} 
\usetheme{Madrid} 
\setbeamertemplate{navigation symbols}{} 

\title{Introduction to Doubly Robust DiD} 
\subtitle{Sant'Anna and Zhao(2020)}
\author{M1 Inoue Jin}
\institute{Hitotsubashi University} 

\begin{document}
%\setlength{\mathindent}{0pt}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}\frametitle{Doubly Robust DiD}
    \begin{itemize}
        \item Doubly Robust DiD was proposed by Sant'Anna and Zhao(2020)
        \item This paper suggests doubly robust estimators for the ATT in DiD research designs.
        \item Authors also state that one can construct doubly robust DiD estimators for the ATT that are also doubly robust for inference.
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Notation}
    \begin{itemize}
        \item $Y_{it}$ : the outcome of interest for unit $i$ at time $t$
        \item $t$ : researchers have access to outcome data in a pre-treated period $t = 0$ and in a post-treatment period $t = 1$.
        \item $D_{it} = 1$ if $i$ is treated before time t and $D_{it} = 0$ otherwise.
        \begin{itemize}
            \item $D_{i0} = 0$ for every $i$ at time $t$ and then, $D_{i} = D_{i1}$
        \end{itemize}
        \item $Y_{it} = D_{i}Y_{it}(1) + (1 - D_{i})Y_{it}(0)$
        \begin{itemize}
            \item $Y_{i0} = Y_{i0}(0)$ for all $i$ 
            \item $Y_{i1} = D_{i}Y_{i1}(1) + (1 - D_{i})Y_{i1}(0)$
        \end{itemize}
        \item $X_{i}$ :a vector of pre-treatment covariates.
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{The parameter of interest}
    \begin{itemize}
        \item Difference in Differences identifies the average treatment effect for the treated (ATT). 
    \end{itemize}
    \begin{block}{The parameter of interest:ATT}
        $\tau = \mathbb{E}[Y_{i1}(1) - Y_{i1}(0)| D_{i} = 1] = \mathbb{E}[Y_{1} |D = 1] - \mathbb{E}[Y_{1}(0) | D = 1]$
    %Y_{i1}(1) = Y_{i1} if D_{i} = 1
    \end{block}
\end{frame}

\begin{frame}\frametitle{Assumption 1.}
    \begin{assumption}[Random Sampling]
        the data $\{Y_{i0}, Y_{i1}, D_{i}, X_{i}\}^{n}_{i = 1}$ are independent and identically distributed (iid)
    \end{assumption}
\end{frame}

\begin{frame}\frametitle{Assumption 2.}
    \begin{assumption}[Conditional PTA]
        $\mathbb{E}[Y_{1}(0) - Y_{0}(0)| D = 1, X] = \mathbb{E}[Y_{1}(0) - Y_{0}(0)|D = 0, X]$ almost surely (a.s.).
    \end{assumption}
    \begin{itemize}
        \item This assumption is called "Conditional Parallel Trend Assumption"
        \item It is crucial for most of DiD literature.
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Assumption 3.}
    \begin{assumption}[Overlap Condition]
        For some $\varepsilon > 0$, $\mathbb{P}(D = 1) > \varepsilon$ and $\mathbb{P}(D = 1|X) \leq 1 - \varepsilon$ a.s.
    \end{assumption}
    \begin{itemize}
        \item This assumption states that at least a small fraction of the population is treated and that for every value of the covariates X, there is at least a small probability that the unit is not treated.
        \item These assumptions are standard in conditional DID methods.
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Doubly Robust DiD estimand:Notation}
    \begin{itemize}
        \item $\pi(X)$ : an arbitrary model for the true, unknown propensity score
        \item $\Delta Y = Y_{1} - Y_{0}$:the difference of observed outcomes
        \item $\mu ^{p}_{d,\Delta} \equiv \mu ^{p}_{d,1}(X) - \mu ^{p}_{d,0}(X)$
        \begin{itemize}
            \item $\mu ^{p}_{d,t}$ : a model for the true, unknown outcome regression $m^{p}_{d,t} \equiv \mathbb{E}[Y_{t}|D = d, X = x]],d,t = 0,1$
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Doubly Robust DiD estimand}
    \begin{block}{DR DID estimand when panel data are available}
        \begin{equation*}
        \tau^{dr,p} = \mathbb{E}\lbrack (w^{p}_{1}(D)- w^{p}_{0}(D,X;\pi))(\Delta Y - \mu ^{p}_{0,\Delta}(X)))\rbrack
        \end{equation*}
        where, for a generic $g$,
        \begin{equation*}
            w^{p}_{1}(D) = \frac{D}{ \mathbb{E}[D]},w^{p}_{0}(D,X;g) = \frac{ \frac{g(X)(1-D)}{1-g(X)}}{ \mathbb{E}[\frac{g(X)(1-D)}{1-g(X)}]}
        \end{equation*}
    \end{block}
\end{frame}

\begin{frame}\frametitle{Doubly Robust DiD estimand}
    \begin{block}{Theorem 1.}
        Let Assumptions 1-3 hold. When panel data are available, $\tau ^{dr,p} = \tau$ if either (but not necessarily both) $\pi(X) = p(X)$ a.s. or $\mu ^{p}_{\Delta}(X) = m^{p}_{0,1}(X) - m^{p}_{0,0}(X)$
    \end{block}

    \begin{itemize}
        \item This theorem states that at least one of the outcome regression model or propensity score model is correctly specified, we can recover the average treatment effect for the treated(ATT).
        \begin{itemize}
            \item Case1:the propensity score model is correctly specified($\pi(X) = p(X) = P(D = 1|X)$), but the outcome regression model is miss-specified.
            \item Case2:the outcome regression model is correcly specified($\mu ^{p}_{0,\Delta}(X) = m^{p}_{0,\Delta}(X) = \mathbb{E}[Y_{1}|D = 0, X] - \mathbb{E}[Y_{0}|D = 0, X]$), but the propensity score model is miss-specified.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Proof of Theorem 1.(Case 1)}
    In Case 1, the propensity score model $\pi(X)$ is equivalent to the true propensity score $p(X) = P(D = 1|X)$. 

    To begin with, calculate the difference of two weights $w^{p}_{1}(D) - w^{p}_{0}(D,X)$:
    \begin{eqnarray*}
        w^{p}_{1}(D) - w^{p}_{0}(D,X) &=& \frac{D}{ \mathbb{E}[D]} - \frac{ \frac{\pi(X)(1-D)}{1-\pi(X)}}{ \mathbb{E}[\frac{\pi(X)(1-D)}{1-\pi(X)}]} \\
        &=& \frac{D}{\mathbb{E}[D]} - \frac{\frac{p(X)(1-D)}{1-p(X)}}{\mathbb{E}[D]} \\
        &=& \frac{1}{\mathbb{E}[D]} \left(\frac{(1-p(X))D}{1-p(X)} - \frac{(1-D)p(X)}{1-p(X)} \right) \\
        &=& \frac{D - p(X)}{\mathbb{E}[D](1-p(X))}
    \end{eqnarray*}
\end{frame}

\begin{frame}\frametitle{Proof of Theorem 1.(Case 1)}
Therefore,
    \begin{eqnarray*}
        \tau ^{dr,p} &=& \mathbb{E}\left[(w^{p}_{1}(D) - w^{p}_{0}(D,X))(\Delta Y - \mu^{p}_{0,\Delta}(X))\right] \\
        &=& \mathbb{E}\left[\left(\frac{D - p(X)}{\mathbb{E}[D](1-p(X))}\right)\left(\Delta Y - \mu^{p}_{0,\Delta}(X)\right)\right] \\
        &=& \mathbb{E}\left[\left(\frac{D - p(X)}{\mathbb{E}[D](1-p(X))}\right)\Delta Y\right] - \mathbb{E}\left[\left(\frac{D - p(X)}{\mathbb{E}[D](1-p(X))}\right)\mu^{p}_{0,\Delta}(X)\right]
    \end{eqnarray*}
The first term is equivalent to the Abadie(2005)'s IPW DID estimator. The second term seems to be "bias term" and we want this term to be zero.
\end{frame}

\begin{frame}\frametitle{Proof of Theorem 1.(Case 1)}
    Then, the bias term is:
    \begin{eqnarray*}
        \mathbb{E}\left[\left(\frac{D-p(X)}{\mathbb{E}[D](1-p(X))}\right)\mu^{p}_{0, \Delta}(X)\right] &=& \mathbb{E}\left[\mathbb{E}\left[\frac{\left(D-p(X)\right)\mu^{p}_{0,\Delta}(X)}{\mathbb{E}[D](1-p(X))}\middle| X\right]\right] \\
        &=& \mathbb{E}\left[\frac{\mu^{p}_{0,\Delta}(X)\mathbb{E}\left[(D-p(X))\middle|X\right]}{\mathbb{E}[D](1-p(X))}\right]\\
        &=& \mathbb{E}\left[\frac{\mu^{p}_{0,\Delta}(X) \left(\mathbb{E}\left[D\middle|X\right] - p(X)\right)}{\mathbb{E}\left[D\right](1-p(X))}\right] \\
        &=& 0
    \end{eqnarray*}
The first line is obtained by the law of iterated expectation, and the third line reduces to 0 because $p(X) = P(D = 1|X) = \mathbb{E}[D|X]$.
\end{frame}

\begin{frame}\frametitle{Proof of Theorem 1.(Case 1)}
    Next, check whether the IPW estimator is equivalent to ATT:
    \begin{eqnarray*}
        \mathbb{E}\left[\frac{(D-p(X))}{\mathbb{E}[D](1-p(X))}\Delta Y \right] &=&\mathbb{E}\left[\mathbb{E}\left[\mathbb{E}\left[\frac{(D-p(X))}{\mathbb{E}[D](1-p(X))}\Delta Y\middle|D,X\right]\middle|X\right]\right] \\
    \end{eqnarray*}

    Note that $\Delta Y = Y_{1} - Y_{0} = DY_{1}(1) + (1-D)Y_{1}(0) - Y_{0}(0)$ (applying the equation of potential outcomes.) \\
\end{frame}

\begin{frame}\frametitle{Proof of Theorem 1.(Case 1)}
    We can calculate the second expectation by definition:
    \footnotesize
    \begin{align*}
        \MoveEqLeft
        \mathbb{E}_{D}\left[\mathbb{E}\left[\frac{D-p(X)}{\mathbb{E}[D](1-p(X))}\left(DY_{1}(1) + (1-D)Y_{1}(0) - Y_{0}(0)\right)\middle|D,X\right]\middle|X \right]
        \\&= 
        \frac{p(X)}{P(D = 1)}\mathbb{E}\left[Y_{1}(1) - Y_{0}(0)\middle|D = 1, X\right] - \frac{p(X)}{P(D = 1)}\mathbb{E}\left[Y_{1}(0) - Y_{0}(0)\middle|D = 0, X \right]
        \\&= 
        \frac{p(X)}{P(D = 1)} \{ \mathbb{E}[Y_{1}(1) - Y_{0}(0)|D = 1, X] - \mathbb{E}[Y_{1}(0) - Y_{0}(0)|D = 1, X] \}
        \\&=
        \frac{p(X)}{P(D = 1)}\mathbb{E}[Y_{1}(1) - Y_{1}(0)|D = 1, X]
    \end{align*}
    \normalsize
    The first equality follows from the direct calculation of the outer conditional expectation, and we can obtain the second equality by applying the conditional PTA(Assumption 2.).
\end{frame}

\begin{frame}\frametitle{Proof of Theorem 1.(Case 1.)}
    Finally, ATT is recovered by direct calculation.
    \begin{align*}
        \MoveEqLeft
        \mathbb{E}_{X}\left[\frac{p(X)}{P(D = 1)}\mathbb{E}[Y_{1}(1) - Y_{1}(0)|D = 1, X]\right]
        \\&=
        \int_{x}\frac{P(D = 1|X)}{P(D = 1)} \int_{y} \left(y_{1}(1) - y_{1}(0)\right)f(y|d = 1, x)f(x)dydx
        \\&=
        \int_{y}\int_{x}\frac{f(d = 1|x)}{f(d = 1)}\frac{f(y, d = 1, x)}{f(d = 1, x)}f(x)\left(y_{1}(1) - y_{1}(0)\right)dxdy
        \\&=
        \int_{y}\int_{x}\frac{f(d = 1|x)}{f(d = 1)}\frac{f(y, d = 1, x)f(x)}{f(d = 1 |x)f(x)}\left(y_{1}(1) - y_{1}(0)\right)dxdy
        \\&=
        \int_{y}\frac{y_{1}(1) - y_{1}(0)}{f(d = 1)} \biggl \{ \int_{x}f(y, d = 1, x)dx \biggl \} dy
        \\&=
        \int_{y}(y_{1}(1) - y_{1}(0))f(y|d = 1)dy
        \\&=
        \mathbb{E}[Y_{1}(1) - Y_{1}(0)|D = 1]
    \end{align*}
\end{frame}

\begin{frame}\frametitle{Estimation and Inference}
    \begin{block}{DR DiD estimator}
        \begin{equation*}
            \widehat{\tau}^{dr,p} = \mathbb{E}_{n}\left[(\widehat{w}^{p}_{1}(D) - \widehat{w}^{p}_{0}(D,X;\widehat{\gamma}))(\Delta Y - \mu^{p}_{0, \Delta}(X;\widehat{\beta}^{p}_{0,0},\widehat{\beta}^{p}_{0,1}))\right],
        \end{equation*}
        where\\
        $\widehat{w}^{p}_{1}(D) = \frac{D}{\mathbb{E}_{n}[D]}$, $\widehat{w}^{p}_{0}(D,X;\gamma) = \left. \frac{\pi(X;\gamma)(1-D)}{1-\pi(X;\gamma)}\middle/ \mathbb{E}_{n}\left[\frac{\pi(X;\gamma)(1-D)}{1-\pi(X;\gamma)}\right]\right.$
    \end{block}
    \begin{itemize}
        \item $\widehat{\tau}^{dr,p}$ is doubly robust, and also locally semiparametrically efficient when the working models for the nuisance functions are correctly specified.(Theorem A.1 in Appendix A)
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Semiparametric efficiency bound}
    \footnotesize
    \begin{itemize}
        \item This result provides the semiparametric analogue of the Cram\'{e}r-Rao lower bound commonly used in fully parametric procedures.
        \item Thus, this provides a benchmark that researchers can use to assess whether any given (regular) semiparametric DiD estimator works well.
    \end{itemize}
    \normalsize
    \begin{block}{Proposition 1}
        \footnotesize
        Let Assumptions 1-3 hold. Then, when panel data are available, the efficient influence function for the ATT is 
        \normalsize
        \footnotesize
        \begin{align*}
            \eta^{e,p}(Y_{1}, Y_{0}, D, X) &= w^{p}_{1}(D)(m^{p}_{1, \Delta}(X) - m^{p}_{0. \Delta}(X) - \tau) \\
            & + w^{p}_{1}(D)(\Delta Y - m^{p}_{1, \Delta}(X)) - w^{p}_{0}(D,X;p)(\Delta Y - m^{p}_{0, \Delta}(X)),
        \end{align*}
        \normalsize
        \footnotesize
        and the semiparametric efficiency bound for all regular estimators for the ATT is 
        \normalsize
        \footnotesize
        \begin{align*}
            \begin{split}
                \mathbb{E}[\eta^{e,p}(Y_{1}, Y_{0}, D, X)^{2}] &= \frac{1}{\mathbb{E}[D]^{2}}\mathbb{E} \biggr[ D(m^{p}_{1, \Delta}(X) - m^{p}_{0,\Delta}(X) - \tau)^{2} 
                \\&+ \left. D(\Delta Y - m^{p}_{1, \Delta}(X))^{2} + \frac{(1-D)p(X)^{2}}{(1-p(X))^{2}}(\Delta Y - m^{p}_{0. \Delta}(X))^{2} \right]
            \end{split}
        \end{align*}
        Note that $m^{p}_{0.\Delta}(X) \equiv m^{p}_{0,1}(X) - m^{p}_{0,0}(X)$
        \normalsize
    \end{block}
\end{frame}


%improved estimator 
\begin{frame}\frametitle{Estimation and Inference}

\end{frame}

\begin{frame}\frametitle{Estimation and Inference}
    \begin{block}{Theorem 2.}
        Suppose Assumptions 1-3 and Assumption A.1-A.2 stated in Appendix A hold, and that the working models are logit model and linear regression model. Then,
        \\(a) If either $\Lambda(X'\gamma^{\ast,ipt}) = p(X)$ a.s. or $X'\beta^{\ast, wls, p}_{0,\Delta} = m^{p}_{0,\Delta}(X)$ a.s., then, as $n \rightarrow \infty$,
        $$
            \widehat{\tau}^{dr,p}_{imp} \overset{p}{\to} \tau
        $$
        and
        \begin{align*}
            \sqrt{n}(\widehat{\tau}^{dr,p}_{imp} - \tau^{dr,p}_{imp})
            &= 
            \frac{1}{\sqrt{n}}\sum_{i = 1}^{n} \eta^{dr,p}_{imp}(W;\gamma^{\ast,ipt}, \beta^{\ast,wls,p}_{0,\Delta},\tau^{dr,p}_{imp}) + o_{p}(1)
            \\ & \overset{d}{\to} N(0,V^{p}_{imp}),
        \end{align*}
        where $V^{p}_{imp} = \mathbb{E}\left[\eta^{dr,p}_{imp}(W;\gamma^{\ast, ipt},\beta^{\ast, wls, p}_{0, \Delta}, \tau^{dr, p}_{imp})^{2}\right]$.
    \end{block}
\end{frame}


\begin{frame}\frametitle{Monte Carlo simulation study}

%simulation result
\end{frame}


%Appendix

\begin{frame}\frametitle{Appendix A}
    \footnotesize
    \begin{block}{Assumption A.1.}
    ($i$) $g(x) = g(x;\theta)$ is a parametric model, where $\theta \in \Theta \subset \mathbb{R}^{k},$ $\Theta$ being compact; ($ii$) $g(x;\theta)$ is $a.s.$ continuous at each $\theta \in \Theta$; ($iii$) there exists a unique pseudo-true parameter $\theta^{\ast} \in int(\Theta)$; ($iv$) $g(x;\theta)$ is $a.s.$ twice continuously differentiable in a neighborhood of $\theta^{\ast}, \Theta^{\ast} \subset \Theta$; ($v$) the estimator $\widehat{\theta}$ is strongly consistent for the $\theta^{\ast}$ and satisfies the following linear expansion:
    \footnotesize
    \begin{equation*}
        \sqrt{n}(\widehat{\theta} - \theta^{\ast}) = \frac{1}{\sqrt{n}} \sum_{i = 1}^{n}l_{g}(W_{i};\theta^{\ast}) + o_{p}(1),
    \end{equation*}
    \normalsize
    \footnotesize
    where $l_{g}(\cdot;\theta)$ is such that $\mathbb{E}[l_{g}(W;\theta^{\ast})] = 0$, $\mathbb{E}[l_{g}(W;\theta^{\ast})l_{g}(W;\theta^{\ast})']$ exists and is positive definite and $ lim_{\delta \to \infty} \mathbb{E}\left[\sup_{\theta \in \Theta^{\ast}:\|\theta-\theta^{\ast}\| \leq \delta}\|l_{g}(W;\theta)-l_{g}(W;\theta^{\ast})\|^{2} \right] = 0$. In addition, ($vi$) for some $\varepsilon > 0$, $0 < \pi(X;\gamma) \leq 1 - \varepsilon \,a.s.$, for all $\gamma \in int(\Theta^{ps})$,where $\Theta^{ps}$ denotes the parameter space of $\gamma$.
    \normalsize
    \end{block}
    \normalsize
\end{frame}

\begin{frame}\frametitle{Appendix A}
    \begin{block}{Assumption A.2.}
        When panel data are available, assume that $\mathbb{E}[\|h^{p}(W;\kappa^{\ast, p})\|^{2}] < \infty$ and $\mathbb{E}[sup_{\kappa \in \Gamma^{\ast, p}}|\dot{h}^{p}(W;\kappa)|] < \infty$,where $\Gamma^{\ast, p}$ is a small neighborhood of $\kappa^{\ast, p}$.
    \end{block}
    %some notations need to be explained,i.e.,\eta^p \kappa,...
    %dot{h} also need to be explained...
\end{frame}

\begin{frame}\frametitle{Appendix A}
    \footnotesize
    \begin{block}{Theorem A.1.}
        Suppose Assumptions 1-3 and Assumptions A.1-A.2 stated in Appendix A hold.
        Consider the following claims:
        \footnotesize
        \begin{align*}
            &\exists \gamma^{\ast} \in \Theta^{ps}:\mathbb{P}(\pi(X;\gamma^{\ast}) =p(X)) = 1,\\
            &\exists (\beta^{\ast, p}_{0,1}, \beta^{\ast, p}_{0,0}) \in \Theta^{reg}_{0,1} \times \Theta^{reg}_{0,0}:\mathbb{P}(\mu^{p}_{0,1}(X;\beta^{\ast,p}_{0,1}) - \mu^{p}_{0,0}(X;\beta^{\ast,p}_{0,0}) = m^{p}_{0,1}(X) - m^{p}_{0,0}(X)) = 1.
        \end{align*}
        \normalsize
        \footnotesize
        (a) Provided that either one of claims stated above is true, as $n \to \infty$,
        \begin{equation*}
            \widehat{\tau}^{dr,p} \overset{p}{\to} \tau.
        \end{equation*}
        Furthermore,
        \normalsize
        \footnotesize
        \begin{align*}
            \sqrt{n}(\widehat{\tau}^{dr,p} - \tau^{dr,p}) &= \frac{1}{\sqrt{n}}\sum_{i = 1}^{n}\eta^{p}(W_{i};\gamma^{\ast},\beta^{\ast,p}_{0}) + o_{p}(1)
            \\& \overset{d}{\to} N(0,V^{p}),
        \end{align*}
        \normalsize
        \footnotesize
        where $V^{p} = \mathbb{E}[\eta^{p}(W;\gamma^{\ast},\beta^{\ast,p}_{0})^{2}]$.\\
        (b) When both of the claims above are true, $\eta^{p}(W;\gamma^{\ast},\beta^{\ast, p}_{0}) = \eta^{e,p}(Y_{1}, Y_{0}, D, X) a.s.$ and $V^{p}$ is equal to the semiparametric efficient bound.
        \normalsize
    \end{block}
    \normalsize
\end{frame}

\end{document}